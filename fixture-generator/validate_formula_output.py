#!/usr/bin/env python3
"""
Validation script to compare our compiler's formula output against Kibana's fixture output.

This script loads the Kibana fixtures generated by LensConfigBuilder and compares them
against our Python compiler's output to ensure we're generating the correct structure.
"""

import json
import sys
from pathlib import Path

# Add parent directory to path to import from src
sys.path.insert(0, str(Path(__file__).parent.parent))

from pydantic import BaseModel

from dashboard_compiler.panels.charts.lens.metrics.compile import compile_lens_metric
from dashboard_compiler.panels.charts.lens.metrics.config import LensMetricTypes


class LensMetricHolder(BaseModel):
    """A holder for metrics to be used in tests."""

    metric: LensMetricTypes


def load_fixture(fixture_path: Path) -> dict:
    """Load a Kibana fixture JSON file."""
    with open(fixture_path) as f:
        return json.load(f)


def extract_columns(fixture: dict) -> dict:
    """Extract the columns from a Kibana fixture."""
    return fixture['state']['datasourceStates']['formBased']['layers']['layer_0']['columns']


def test_simple_arithmetic():
    """Test simple arithmetic formula: (count(kql='status:error') / count()) * 100"""
    print("\n=== Testing Simple Arithmetic Formula ===")

    # Load Kibana fixture
    fixture_path = Path(__file__).parent / 'output' / 'v9.2.0' / 'metric-formula-simple-arithmetic-dataview.json'
    fixture = load_fixture(fixture_path)
    kibana_columns = extract_columns(fixture)

    print(f"Kibana columns: {list(kibana_columns.keys())}")
    print(f"Kibana formula column:")
    print(json.dumps(kibana_columns['metric_formula_accessor'], indent=2))

    # Compile with our compiler
    metric_holder = LensMetricHolder.model_validate({
        'metric': {
            'formula': {
                'multiply': {
                    'left': {
                        'divide': {
                            'left': {'count': {'filter': {'kql': 'status:error'}}},
                            'right': {'count': {}},
                        }
                    },
                    'right': 100,
                }
            },
            'label': 'Error Rate %',
        }
    })

    _metric_id, primary_column, helper_columns = compile_lens_metric(metric=metric_holder.metric)

    print(f"\nOur columns: primary + {len(helper_columns)} helpers")
    print(f"Our primary column:")
    print(json.dumps(primary_column.model_dump(), indent=2))

    if len(helper_columns) > 0:
        print(f"\nOur helper columns: {list(helper_columns.keys())}")
        for col_id, col in helper_columns.items():
            print(f"\n{col_id}:")
            print(json.dumps(col.model_dump(), indent=2))

    # Compare
    print("\n=== Comparison ===")
    kibana_formula = kibana_columns['metric_formula_accessor']['params']['formula']
    our_formula = primary_column.params.formula

    print(f"Kibana formula: {kibana_formula}")
    print(f"Our formula: {our_formula}")
    print(f"Formula match: {kibana_formula == our_formula}")

    print(f"\nKibana references: {kibana_columns['metric_formula_accessor']['references']}")
    our_references = primary_column.model_dump().get('references', [])
    print(f"Our references: {our_references}")

    if len(kibana_columns['metric_formula_accessor']['references']) == 0 and len(our_references) > 0:
        print("\n❌ MISMATCH: Kibana uses no helper columns, but we generate helper columns!")
        return False

    return True


def test_field_aggregations():
    """Test field aggregations formula: (max - min) / avg"""
    print("\n=== Testing Field Aggregations Formula ===")

    # Load Kibana fixture
    fixture_path = Path(__file__).parent / 'output' / 'v9.2.0' / 'metric-formula-field-aggregations-dataview.json'
    fixture = load_fixture(fixture_path)
    kibana_columns = extract_columns(fixture)

    print(f"Kibana columns: {list(kibana_columns.keys())}")
    print(f"Kibana formula: {kibana_columns['metric_formula_accessor']['params']['formula']}")
    print(f"Kibana references: {kibana_columns['metric_formula_accessor']['references']}")

    # Compile with our compiler
    metric_holder = LensMetricHolder.model_validate({
        'metric': {
            'formula': {
                'divide': {
                    'left': {
                        'subtract': {
                            'left': {'max': {'field': 'response.time'}},
                            'right': {'min': {'field': 'response.time'}},
                        }
                    },
                    'right': {'average': {'field': 'response.time'}},
                }
            },
        }
    })

    _metric_id, primary_column, helper_columns = compile_lens_metric(metric=metric_holder.metric)

    our_formula = primary_column.params.formula
    print(f"\nOur formula: {our_formula}")
    print(f"Our helper columns: {len(helper_columns)}")

    if len(kibana_columns['metric_formula_accessor']['references']) == 0 and len(helper_columns) > 0:
        print("\n❌ MISMATCH: Kibana uses no helper columns, but we generate helper columns!")
        return False

    return True


def main():
    """Run all validation tests."""
    results = []

    try:
        results.append(('Simple Arithmetic', test_simple_arithmetic()))
    except Exception as e:
        print(f"\n❌ Simple Arithmetic test failed: {e}")
        import traceback
        traceback.print_exc()
        results.append(('Simple Arithmetic', False))

    try:
        results.append(('Field Aggregations', test_field_aggregations()))
    except Exception as e:
        print(f"\n❌ Field Aggregations test failed: {e}")
        import traceback
        traceback.print_exc()
        results.append(('Field Aggregations', False))

    print("\n" + "="*60)
    print("VALIDATION SUMMARY")
    print("="*60)
    for test_name, passed in results:
        status = "✅ PASS" if passed else "❌ FAIL"
        print(f"{status}: {test_name}")

    all_passed = all(passed for _, passed in results)
    if not all_passed:
        print("\n⚠️  Some tests failed - our implementation may not match Kibana's structure")
        return 1
    else:
        print("\n✅ All tests passed!")
        return 0


if __name__ == '__main__':
    sys.exit(main())
